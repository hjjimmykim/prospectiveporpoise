{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hjjimmykim/prospectiveporpoise/blob/master/Untitled0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yqVAD3zsUzb7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Base iterated prisoner's dilemma game"
      ]
    },
    {
      "metadata": {
        "id": "YjYwbRfHUrrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ]
    },
    {
      "metadata": {
        "id": "N4ZntShaPaC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Standard\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# cuda\n",
        "use_cuda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJYoceXaSPXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "iGsR8Ug0SAq1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reward structure\n",
        "T = 5 # Temptation\n",
        "R = 3 # Reward\n",
        "P = 1 # Penalty\n",
        "S = 0 # Sucker\n",
        "\n",
        "# Reward matrix\n",
        "RM = np.zeros([2,2,2])\n",
        "RM[0][0][0] = R\n",
        "RM[0][0][1] = R\n",
        "RM[0][1][0] = S\n",
        "RM[0][1][1] = T\n",
        "RM[1][0][0] = T\n",
        "RM[1][0][1] = S\n",
        "RM[1][1][0] = P\n",
        "RM[1][1][1] = P\n",
        "\n",
        "# RL\n",
        "gamma = 0.95\n",
        "alpha = 0.1\n",
        "\n",
        "# Tensorflow stuff\n",
        "n_eps = 100       # Number of episodes\n",
        "n_turns = 10      # Number of turns per episode\n",
        "n_hidden  = 10\n",
        "layers = 1\n",
        "epsilon = 0.1       # Exploration parameter (epsilon-greedy)\n",
        "epslion_dr = 0.9999\n",
        "input_dim = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eX1tZFMXYgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0-layer Deep Q-Network"
      ]
    },
    {
      "metadata": {
        "id": "nUf98klVXa5p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Q-value table\n",
        "Q_1 = [[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
        "Q_2 = [[0,0],[0,0],[0,0],[0,0],[0,0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMkuQ4ZGaB0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "dc47cc36-93bc-45d7-ea00-fc6ae6b62302"
      },
      "cell_type": "code",
      "source": [
        "class RNN():\n",
        "  \n",
        "  def __init__(self, input_dim, n_hidden):\n",
        "    # x = e.g. [[2,2], [1,0], ...]\n",
        "    # h = LSTM(x) (1 x n_hidden)\n",
        "    # output = Wh + b (1 x 2)\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    \n",
        "    # Weight initialization\n",
        "    self.W = tf.Variable(tf.random_normal([n_hidden, input_dim]))\n",
        "    self.b = tf.Variable(tf.random_normal([input_dim]))\n",
        "    \n",
        "    self.rnn_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # generate prediction\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # there are input_dim outputs but\n",
        "    # we only want the last output\n",
        "    return tf.matmul(outputs[-1], W) + b\n",
        "  \n",
        "a = np.zeros([3,2])\n",
        "a[0][0] = 2\n",
        "a[1][1] = 3\n",
        "a[2][0] = 5\n",
        "print(a)\n",
        "\n",
        "print(np.amax(a,axis=1))\n",
        "\n",
        "b = np.zeros([1,2])\n",
        "np.concatenate((a[1:,:],np.zeros([1,2])),axis=0)\n",
        "\n",
        "c = np.array([1,2])\n",
        "print(c)\n",
        "c = [c]\n",
        "print(np.flip(c,axis=0))\n",
        "print(c[-1][::-1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2. 0.]\n",
            " [0. 3.]\n",
            " [5. 0.]]\n",
            "[2. 3. 5.]\n",
            "[1 2]\n",
            "[[1 2]]\n",
            "[2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pUxFvSmTUOfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Game"
      ]
    },
    {
      "metadata": {
        "id": "K3POCMTwgz6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "44e1a061-818d-4635-bd17-ffee8cf9b957"
      },
      "cell_type": "code",
      "source": [
        "sess = tf.InteractiveSession() # Initialize session\n",
        "\n",
        "# Q-networks\n",
        "RNN1 = RNN(input_dim, n_hidden)\n",
        "RNN2 = RNN(input_dim, n_hidden)\n",
        "\n",
        "nextQ1 = tf.placeholder(shape=[n_turns,2],dtype=tf.float32)\n",
        "x1 = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
        "Qout1 = RNN1.forward(x)\n",
        "\n",
        "nextQ2 = tf.placeholder(shape=[n_turns,2],dtype=tf.float32)\n",
        "x2 = tf.placeholder(shape=[1,2],dtype=tf.float32)\n",
        "Qout2 = RNN2.forward(x)\n",
        "\n",
        "# Loss and optimizer\n",
        "cost1 = tf.reduce_sum(tf.square(nextQ1-Qout1))\n",
        "optimizer1 = tf.train.RMSPropOptimizer(learning_rate=alpha).minimize(cost1)\n",
        "\n",
        "cost2 = tf.reduce_sum(tf.square(nextQ2-Qout2))\n",
        "optimizer2 = tf.train.RMSPropOptimizer(learning_rate=alpha).minimize(cost2)\n",
        "\n",
        "sess.run(tf.global_variables_initializer()) # Initialize variables\n",
        "\n",
        "for i_ep in range(n_eps):\n",
        "  # History of outcomes\n",
        "  history = [np.array([2,2])] # Start\n",
        "  \n",
        "  # Keep track of calculated Q-values\n",
        "  Q_pred1 = np.zeros([n_turns,2])\n",
        "  Q_pred2 = np.zeros([n_turns,2])\n",
        "  \n",
        "  # Keep track of rewards\n",
        "  rewards1 = np.zeros([n_turns,1])\n",
        "  rewards2 = np.zeros([n_turns,1])\n",
        "  \n",
        "  for i_turn in range(n_turns):\n",
        "    \n",
        "    # Calculate Q-values\n",
        "    Q1 = sess.run(Qout, feed_dict={x: history, agentID: 1})\n",
        "    Q2 = sess.run(Qout, feed_dict={x: [outcome[::-1] for outcome in history], agentID: 2})\n",
        "    \n",
        "    Q_pred1[i_turn] = Q1\n",
        "    Q_pred2[i_turn] = Q2\n",
        "    \n",
        "    # Epsilon-greedy sampling\n",
        "    if np.random.rand(1) < epsilon:\n",
        "      action1 = np.random.randint(2)\n",
        "    else:\n",
        "      action1 = np.argmax(Q1)\n",
        "      \n",
        "    if np.random.rand(1) < epsilon:\n",
        "      action2 = np.random.randint(2)\n",
        "    else:\n",
        "      action2 = np.argmax(Q2)\n",
        "      \n",
        "    # Get rewards\n",
        "    reward1 = RM[action1][action2][0]\n",
        "    reward2 = RM[action1][action2][1]\n",
        "    \n",
        "    rewards1[i_turn] = reward1\n",
        "    rewards2[i_turn] = reward2\n",
        "    \n",
        "    # Add to history\n",
        "    history.append([action1,action2])\n",
        "    \n",
        "  # Q-learning\n",
        "  \n",
        "  # Q_{s+1}\n",
        "  Q_pred_shift1 = np.concatenate((Q_pred1[1:,:],np.zeros([1,2])),axis=0)\n",
        "  Q_target1 = rewards1 + gamma * np.amax(Q_pred_shift1,axis=1)\n",
        "  \n",
        "  Q_pred_shift2 = np.concatenate((Q_pred2[1:,:],np.zeros([1,2])),axis=0)\n",
        "  Q_target2 = rewards2 + gamma * np.amax(Q_pred_shift2,axis=1)\n",
        "  \n",
        "  sess.run(optimizer1, feed_dict={x1:history, nextQ1:Q_target1})\n",
        "  sess.run(optimizer2, feed_dict={x2:[outcome[::-1] for outcome in history], nextQ2:Q_target2})"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-88f2b5fa9fd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0magentID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mQout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magentID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Loss and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-fa5f57b58e11>\u001b[0m in \u001b[0;36mRNN\u001b[0;34m(x, weights, biases, agentID)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m# reshape to [1, n_input]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;31m# Generate a n_input-element sequence of inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_input' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "krkJucU8Pg3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b283689f-c333-4ad9-b302-a7a77ab43bd2"
      },
      "cell_type": "code",
      "source": [
        "# Previous outcome lookup policy for both players\n",
        "p_1 = [0.3, 0.4, 0.5, 0.2]\n",
        "p_2 = [0.4, 0.3, 0.2, 0.5] # Daniel's pick\n",
        "\n",
        "# Reward for both players\n",
        "R1 = 0\n",
        "R2 = 0\n",
        "\n",
        "outcome = 4 # 0 = CC, 1 = CD, 2 = DC, 3 = DD, 4 = Start\n",
        "for n in range(N):\n",
        "  \n",
        "  # Boltzmann sampling using Q-values\n",
        "  # Probability of collaborating\n",
        "  p_1 = np.exp(Q_1[outcome][0])/np.sum(np.exp(Q_1[outcome]))\n",
        "  p_2 = np.exp(Q_2[outcome][0])/np.sum(np.exp(Q_2[outcome]))\n",
        "  \n",
        "  r1 = np.random.rand()\n",
        "  r2 = np.random.rand()\n",
        "  \n",
        "  if p_1 > r1 and p_2 > r2:\n",
        "    \n",
        "    next_outcome = 0\n",
        "    action1 = 0\n",
        "    action2 = 0\n",
        "    reward1 = R\n",
        "    reward2 = R\n",
        "    \n",
        "  elif p_1 > r1 and p_2 <= r2:\n",
        "    \n",
        "    next_outcome = 1\n",
        "    action1 = 0\n",
        "    action2 = P\n",
        "    reward1 = 0\n",
        "    reward2 = T\n",
        "    \n",
        "  elif p_1 <= r1 and p_2 > r2:\n",
        "    \n",
        "    next_outcome = 2\n",
        "    action1 = 1\n",
        "    action2 = 0\n",
        "    reward1 = T\n",
        "    reward2 = S\n",
        "    \n",
        "  elif p_1 <= r1 and p_2 <= r2:\n",
        "    \n",
        "    next_outcome = 3\n",
        "    action1 = 1\n",
        "    action2 = 1\n",
        "    reward1 = P\n",
        "    reward2 = P\n",
        "    \n",
        "  R1 += reward1\n",
        "  R2 += reward2\n",
        "    \n",
        "  # Q-learning <--- HERE\n",
        "  Q_1[outcome][action1] += alpha * (reward1 + gamma*np.max(Q_1[next_outcome]) - Q_1[outcome][action1])\n",
        "  Q_2[outcome][action2] += alpha * (reward2 + gamma*np.max(Q_2[next_outcome]) - Q_2[outcome][action2])\n",
        "  \n",
        "  outcome = next_outcome\n",
        "    \n",
        "print(R1)\n",
        "print(R2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1021\n",
            "1126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9s1AotMyU7nJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b37f2dca-2552-4ea0-c482-6389751cec67"
      },
      "cell_type": "code",
      "source": [
        "Q_1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.9433724908263161, 0.5659474045908255],\n",
              " [1.0956443850857551, 2.757813922629177],\n",
              " [0.5001720490563554, 0.6315677743814965],\n",
              " [1.7888472409760967, 9.999341552075956],\n",
              " [0, 0.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "1QOyPf8ehwqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p_1 = np.exp(Q_1[outcome][0])/np.sum(np.exp(Q_1[outcome]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1AABBsGhyq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fdf307f-38c7-493f-a5f0-0e0f0b1da30d"
      },
      "cell_type": "code",
      "source": [
        "p_1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00027171249332988586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "UFNg0zpNh9w6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88785425-f8e0-41f5-8b84-49c4992d7af0"
      },
      "cell_type": "code",
      "source": [
        "outcome"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ndELceuxiA97",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wq7-NlkU0acf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Message Network"
      ]
    },
    {
      "metadata": {
        "id": "1uvmotxH0Vjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86505e46-bb5c-4c6e-8ee0-ed8109698130"
      },
      "cell_type": "code",
      "source": [
        "#Create a message\n",
        "#First position is what you did last turn: 0 = c, 1 = d\n",
        "#Next n many characters are a message from each agent\n",
        "#m1 is agent 1 message and m2 is agent 2 message\n",
        "\n",
        "n = 3\n",
        "m1 = np.zeros(n+2)\n",
        "m2 = np.zeros(n+2)\n",
        "\n",
        "m1[0] = action1\n",
        "m2[0] = action2\n",
        "m1[1] = action2\n",
        "m2[2] = action1\n",
        "\n",
        "print(m1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KIM1DTq41z8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWmcts1jaL4H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "6mwRtJEV18ZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}