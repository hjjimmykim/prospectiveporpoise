{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hjjimmykim/prospectiveporpoise/blob/master/ExactValueLearning.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "yqVAD3zsUzb7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "LOLA\n",
        "\n",
        "Ideas:\n",
        "1. RL vs. RL (multiagent)\n",
        "2. Communication channel (pre-set? deception?)\n",
        "3. LSTM stacked on FWN\n",
        "4. Induce RL to learn ZD-strategy (learn probabilities?)\n",
        "5. RL that adapts to opponent's non-stationary strategy"
      ]
    },
    {
      "metadata": {
        "id": "YjYwbRfHUrrr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ]
    },
    {
      "metadata": {
        "id": "N4ZntShaPaC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Standard\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# cuda\n",
        "use_cuda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kJYoceXaSPXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "iGsR8Ug0SAq1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reward structure\n",
        "T = 5 # Temptation\n",
        "R = 3 # Reward\n",
        "P = 1 # Penalty\n",
        "S = 0 # Sucker\n",
        "\n",
        "# Reward matrix (0 = cooperate, 1 = defect)\n",
        "RM = np.zeros([2,2,2])\n",
        "RM[0][0][0] = R\n",
        "RM[0][0][1] = R\n",
        "RM[0][1][0] = S\n",
        "RM[0][1][1] = T\n",
        "RM[1][0][0] = T\n",
        "RM[1][0][1] = S\n",
        "RM[1][1][0] = P\n",
        "RM[1][1][1] = P\n",
        "\n",
        "# RL\n",
        "gamma = 1\n",
        "alpha = 0.01\n",
        "\n",
        "# Tensorflow stuff\n",
        "n_eps = 5200       # Number of episodes\n",
        "n_turns = 10      # Number of turns per episode\n",
        "n_hidden  = 5\n",
        "layers = 1\n",
        "epsilon = 0.5       # Exploration parameter (epsilon-greedy)\n",
        "epsilon_dr = 0.9999\n",
        "input_dim = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eX1tZFMXYgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 0-layer Deep Q-Network"
      ]
    },
    {
      "metadata": {
        "id": "rMkuQ4ZGaB0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "5dbf8953-13fb-44d9-a808-f51015589073"
      },
      "cell_type": "code",
      "source": [
        "# 1. Experience replay\n",
        "# 2. Clamped Q-network\n",
        "\n",
        "tf.reset_default_graph()\n",
        "  \n",
        "class FWN():\n",
        "  \n",
        "  def __init__(self, input_dim, n_hidden, output_dim):\n",
        "    # x = e.g. [[0,0], [-1,1], ...]\n",
        "    # h = LSTM(x) (1 x n_hidden)\n",
        "    # output = Wh + b (1 x 2)\n",
        "    \n",
        "    self.input_dim = input_dim\n",
        "    \n",
        "    # Weight initialization\n",
        "    self.W1 = tf.Variable(tf.random_normal([input_dim, n_hidden]))\n",
        "    self.b1 = tf.Variable(tf.random_normal([n_hidden]))\n",
        "    \n",
        "    self.W2 = tf.Variable(tf.random_normal([n_hidden, output_dim]))\n",
        "    self.b2 = tf.Variable(tf.random_normal([output_dim]))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = tf.matmul(x, self.W1) + self.b1\n",
        "    x = tf.nn.sigmoid(x)\n",
        "    \n",
        "    x = tf.matmul(x, self.W2) + self.b2\n",
        "    \n",
        "    return x\n",
        "\n",
        "#tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession() # Initialize session\n",
        "\n",
        "# Model 1 ----------------------------------------------------------------------\n",
        "\n",
        "# Define Q-networks\n",
        "model1 = FWN(2, n_hidden, 1)\n",
        "\n",
        "# Target network\n",
        "model1_target = FWN(2, n_hidden, 1)\n",
        "\n",
        "# Copy weights\n",
        "model1_target.W1 = model1.W1 + 0\n",
        "model1_target.b1 = model1.b1 + 0\n",
        "model1_target.W2 = model1.W2 + 0\n",
        "model1_target.b2 = model1.b2 + 0\n",
        "\n",
        "# Feedforward operation (FWN)\n",
        "x1 = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
        "Qout1 = model1.forward(x1)\n",
        "\n",
        "Qout1_target = model1_target.forward(x1)\n",
        "\n",
        "action_history1 = tf.placeholder(shape=(),dtype=tf.int32) # Chosen action\n",
        "Qout1_train = model1.forward(x1)\n",
        "Qout1_train = Qout1_train[0][action_history1] # Q-estimate for the selected action\n",
        "\n",
        "# Loss and optimizer\n",
        "nextQ1 = tf.placeholder(shape=[1,1],dtype=tf.float32) # Empirical\n",
        "cost1 = tf.reduce_sum(tf.square(nextQ1-Qout1_train))\n",
        "optimizer1 = tf.train.GradientDescentOptimizer(learning_rate=alpha).minimize(cost1)\n",
        "\n",
        "#cost2 = tf.reduce_sum(tf.square(nextQ2-Qout2))\n",
        "#optimizer2 = tf.train.RMSPropOptimizer(learning_rate=alpha).minimize(cost2)\n",
        "\n",
        "# Model 2 ----------------------------------------------------------------------\n",
        "model2 = FWN(2, n_hidden, 5)\n",
        "\n",
        "# Target network\n",
        "model2_target = FWN(2, n_hidden, 5)\n",
        "\n",
        "# Copy weights\n",
        "model2_target.W1 = model2.W1 + 0\n",
        "model2_target.b1 = model2.b1 + 0\n",
        "model2_target.W2 = model2.W2 + 0\n",
        "model2_target.b2 = model2.b2 + 0\n",
        "\n",
        "# Feedforward operation (FWN)\n",
        "x2 = tf.placeholder(shape=[None,2],dtype=tf.float32)\n",
        "Qout2 = model2.forward(x2)\n",
        "\n",
        "Qout2_target = model2_target.forward(x2)\n",
        "\n",
        "action_history2 = tf.placeholder(shape=(),dtype=tf.int32) # Chosen action\n",
        "Qout2_train = model2.forward(x2)\n",
        "Qout2_train = Qout2_train[0][action_history2]\n",
        "\n",
        "# Loss and optimizer\n",
        "nextQ2 = tf.placeholder(shape=[1,1],dtype=tf.float32)\n",
        "cost2 = tf.reduce_sum(tf.square(nextQ2-Qout2_train))\n",
        "optimizer2 = tf.train.GradientDescentOptimizer(learning_rate=alpha).minimize(cost2)\n",
        "\n",
        "sess.run(tf.global_variables_initializer()) # Initialize variables\n",
        "\n",
        "#blah = sess.run(Qout1, feed_dict={x1:np.reshape(np.array([-1,-1,-1,-1]),[1,4])})\n",
        "#print(blah)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "pUxFvSmTUOfL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Game"
      ]
    },
    {
      "metadata": {
        "id": "K3POCMTwgz6T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "0d88d307-aa3c-4f5e-c6c3-43e524a27f15"
      },
      "cell_type": "code",
      "source": [
        "for i_ep in range(n_eps):\n",
        "  # History of outcomes (states -1 = unknown, 0 = coop, 1 = defect)\n",
        "  history = -np.ones([n_turns,2]) # Start\n",
        "  \n",
        "  # Keep track of calculated Q-values\n",
        "  Q_pred1 = np.zeros([n_turns,2])\n",
        "  Q_pred2 = np.zeros([n_turns,2])\n",
        "  \n",
        "  # Keep track of rewards\n",
        "  rewards1 = np.zeros([n_turns,1])\n",
        "  rewards2 = np.zeros([n_turns,1])\n",
        "  \n",
        "  for i_turn in range(n_turns):\n",
        "    \n",
        "    # Calculate Q-values\n",
        "    state1 = history\n",
        "    state2 = np.flip(state1,axis=1)\n",
        "    \n",
        "    # For feedforward\n",
        "    state1 = np.reshape(state1,[1,2])\n",
        "    state2 = np.reshape(state2,[1,2])\n",
        "\n",
        "    # Action selection\n",
        "    Q1 = sess.run(Qout1, feed_dict={x1: state1})\n",
        "    \n",
        "    # 0 = cooperate, 1 = defect\n",
        "    if np.random.rand(1) < Q1:\n",
        "      action1 = 0\n",
        "    else:\n",
        "      action1 = 1\n",
        "      \n",
        "    Q2 = sess.run(Qout2, feed_dict={x2: state2})\n",
        "    \n",
        "    # 0 = cooperate, 1 = defect\n",
        "    if np.random.rand(1) < Q2:\n",
        "      action2 = 0\n",
        "    else:\n",
        "      action2 = 1\n",
        "\n",
        "    '''\n",
        "    # Tit-for-tat with initial collab\n",
        "    #if i_turn == 0 or history[i_turn-1][0] == 0:\n",
        "    #  action2 = 0\n",
        "    #else:\n",
        "    #  action2 = 1\n",
        "    \n",
        "    # Uniform action\n",
        "    #action2 = 0\n",
        "    \n",
        "    # Win-stay lose-shift with initial collab\n",
        "    if i_turn == 0:\n",
        "      action2 = 0\n",
        "    elif reward2 == T or reward2 == R:\n",
        "      action2 = action2\n",
        "    elif reward2 == S or reward2 == P:\n",
        "      action2 = int(not action2)\n",
        "    '''\n",
        "      \n",
        "    # Get rewards\n",
        "    reward1 = RM[action1][action2][0]\n",
        "    reward2 = RM[action1][action2][1]\n",
        "    \n",
        "    rewards1[i_turn] = reward1\n",
        "    rewards2[i_turn] = reward2\n",
        "    \n",
        "    # Next state\n",
        "    history_next = copy.deepcopy(history)\n",
        "    history_next[i_turn] = [action1, action2]\n",
        "    state1_next = np.reshape(history_next[i_turn],[1,2])\n",
        "    state2_next = np.reshape(np.flip(history_next[i_turn],axis=1),[1,2])\n",
        "    \n",
        "    # Q-learning (FWN)\n",
        "    Q1_next = sess.run(Qout1_target, feed_dict={x1: state1_next})\n",
        "\n",
        "    Q_target1 = reward1 + gamma * np.amax(Q1_next,axis=1,keepdims=1)\n",
        "    \n",
        "    Q2_next = sess.run(Qout2_target, feed_dict={x2: state2_next})\n",
        "\n",
        "    Q_target2 = reward2 + gamma * np.amax(Q2_next,axis=1,keepdims=1)\n",
        "    \n",
        "    # Last turn\n",
        "    if i_turn == n_turns - 1:\n",
        "      Q_target1 = np.reshape(reward1,[1,1])\n",
        "      Q_target2 = np.reshape(reward2,[1,1])\n",
        "    \n",
        "    sess.run(optimizer1, feed_dict={x1:state1, nextQ1:Q_target1, action_history1:action1})\n",
        "    sess.run(optimizer2, feed_dict={x2:state2, nextQ2:Q_target2, action_history2:action2})\n",
        "    \n",
        "    # Update history\n",
        "    history = copy.deepcopy(history_next)\n",
        "  \n",
        "  # Q_{s+1}\n",
        "  if i_ep % 100 == 0:\n",
        "    print(np.sum(rewards1))\n",
        "    print(np.sum(rewards2))\n",
        "    print(history)\n",
        "    \n",
        "    # Update target network\n",
        "    # Copy weights\n",
        "    model1_target.W1 = model1.W1 + 0\n",
        "    model1_target.b1 = model1.b1 + 0\n",
        "    model1_target.W2 = model1.W2 + 0\n",
        "    model1_target.b2 = model1.b2 + 0\n",
        "    \n",
        "    model2_target.W1 = model2.W1 + 0\n",
        "    model2_target.b1 = model2.b1 + 0\n",
        "    model2_target.W2 = model2.W2 + 0\n",
        "    model2_target.b2 = model2.b2 + 0\n",
        "\n",
        "  # Epsilon-greedy schedule\n",
        "  epsilon = epsilon * epsilon_dr\n",
        "  if i_ep == 5000:\n",
        "    print(epsilon)\n",
        "    epsilon = 0\n",
        "    print('------------------------------------------------')\n",
        "  \n",
        "#sess.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-24fcd384851d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Action selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Epsilon-greedy sampling; 0 = cooperate, 1 = defect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Qout1' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "77UFkT1drilN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "8e2a4337-0815-4ec4-c51e-6f85107405d7"
      },
      "cell_type": "code",
      "source": [
        "state1 = np.array([[-1,-1],[-1,-1]])\n",
        "state1 = np.reshape(state1,[1,4])\n",
        "print(state1)\n",
        "Q1 = sess.run(Qout1, feed_dict={x1: state1})\n",
        "print(Q1)\n",
        "\n",
        "state1 = np.array([[0,0],[-1,-1]])\n",
        "state1 = np.reshape(state1,[1,4])\n",
        "print(state1)\n",
        "Q1 = sess.run(Qout1, feed_dict={x1: state1})\n",
        "print(Q1)\n",
        "\n",
        "state1 = np.array([[1,0],[-1,-1]])\n",
        "state1 = np.reshape(state1,[1,4])\n",
        "print(state1)\n",
        "Q1 = sess.run(Qout1, feed_dict={x1: state1})\n",
        "print(Q1)\n",
        "\n",
        "state1 = np.array([[0,1],[-1,-1]])\n",
        "state1 = np.reshape(state1,[1,4])\n",
        "print(state1)\n",
        "Q1 = sess.run(Qout1, feed_dict={x1: state1})\n",
        "print(Q1)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1 -1 -1 -1]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-85859b3dcf79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstate1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mQ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (1, 4) for Tensor 'Placeholder:0', which has shape '(?, 20)'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "krkJucU8Pg3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "16b56b04-0596-4c94-f6a0-63a2b881bf19"
      },
      "cell_type": "code",
      "source": [
        "history"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "metadata": {
        "id": "9s1AotMyU7nJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b37f2dca-2552-4ea0-c482-6389751cec67"
      },
      "cell_type": "code",
      "source": [
        "Q_1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.9433724908263161, 0.5659474045908255],\n",
              " [1.0956443850857551, 2.757813922629177],\n",
              " [0.5001720490563554, 0.6315677743814965],\n",
              " [1.7888472409760967, 9.999341552075956],\n",
              " [0, 0.1]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "1QOyPf8ehwqN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p_1 = np.exp(Q_1[outcome][0])/np.sum(np.exp(Q_1[outcome]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1AABBsGhyq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fdf307f-38c7-493f-a5f0-0e0f0b1da30d"
      },
      "cell_type": "code",
      "source": [
        "p_1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00027171249332988586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "UFNg0zpNh9w6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88785425-f8e0-41f5-8b84-49c4992d7af0"
      },
      "cell_type": "code",
      "source": [
        "outcome"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "ndELceuxiA97",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wq7-NlkU0acf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Message Network"
      ]
    },
    {
      "metadata": {
        "id": "1uvmotxH0Vjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86505e46-bb5c-4c6e-8ee0-ed8109698130"
      },
      "cell_type": "code",
      "source": [
        "#Create a message\n",
        "#First position is what you did last turn: 0 = c, 1 = d\n",
        "#Next n many characters are a message from each agent\n",
        "#m1 is agent 1 message and m2 is agent 2 message\n",
        "\n",
        "n = 3\n",
        "m1 = np.zeros(n+2)\n",
        "m2 = np.zeros(n+2)\n",
        "\n",
        "m1[0] = action1\n",
        "m2[0] = action2\n",
        "m1[1] = action2\n",
        "m2[2] = action1\n",
        "\n",
        "print(m1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KIM1DTq41z8F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWmcts1jaL4H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "6mwRtJEV18ZV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}